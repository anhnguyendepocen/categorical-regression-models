<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Categorical Models" />
<meta property="og:type" content="book" />


<meta property="og:description" content="description" />
<meta name="github-repo" content="repohere" />

<meta name="author" content="Michael Clark" />

<meta name="date" content="2016-09-18" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="description">

<title>Categorical Models</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-0.7/htmlwidgets.js"></script>
<script src="libs/jquery-1.12.4/jquery.min.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-bootstrap-1.10.12/css/dataTables.bootstrap.min.css" rel="stylesheet" />
<link href="libs/dt-core-bootstrap-1.10.12/css/dataTables.bootstrap.extra.css" rel="stylesheet" />
<script src="libs/dt-core-bootstrap-1.10.12/js/jquery.dataTables.min.js"></script>
<script src="libs/dt-core-bootstrap-1.10.12/js/dataTables.bootstrap.min.js"></script>
<link href="libs/plotlyjs-1.10.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.10.1/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-3.6.0/plotly.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="style_for_miles_and_miles_so_much_style_that_its_wasted.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface"><span class="toc-section-number">1</span> Preface</a></li>
<li><a href="2-intro.html#intro"><span class="toc-section-number">2</span> Introduction</a></li>
<li class="has-sub"><a href="3-two-categories.html#two-categories"><span class="toc-section-number">3</span> Two categories</a><ul>
<li class="has-sub"><a href="3-1-data.html#data"><span class="toc-section-number">3.1</span> Data</a><ul>
<li><a href="3-1-data.html#binomial-distribution"><span class="toc-section-number">3.1.1</span> Binomial Distribution</a></li>
</ul></li>
<li><a href="3-2-logistic-regression-model.html#logistic-regression-model"><span class="toc-section-number">3.2</span> Logistic Regression Model</a></li>
<li><a href="3-3-example.html#example"><span class="toc-section-number">3.3</span> Example</a></li>
<li class="has-sub"><a href="3-4-interpretation.html#interpretation"><span class="toc-section-number">3.4</span> Interpretation</a><ul>
<li><a href="3-4-interpretation.html#odds-ratios"><span class="toc-section-number">3.4.1</span> Odds ratios</a></li>
<li><a href="3-4-interpretation.html#predicted-probabilities"><span class="toc-section-number">3.4.2</span> Predicted probabilities</a></li>
</ul></li>
<li><a href="3-5-summary-of-standard-logistic-regression.html#summary-of-standard-logistic-regression"><span class="toc-section-number">3.5</span> Summary of Standard Logistic Regression</a></li>
<li class="has-sub"><a href="3-6-extensions.html#extensions"><span class="toc-section-number">3.6</span> Extensions</a><ul>
<li><a href="3-6-extensions.html#counts"><span class="toc-section-number">3.6.1</span> Counts</a></li>
<li><a href="3-6-extensions.html#conditional-logistic"><span class="toc-section-number">3.6.2</span> Conditional Logistic</a></li>
<li><a href="3-6-extensions.html#bradley-terry-model"><span class="toc-section-number">3.6.3</span> Bradley-Terry Model</a></li>
</ul></li>
</ul></li>
<li><a href="4-test-page.html#test-page"><span class="toc-section-number">4</span> test page</a></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="logistic-regression-model" class="section level2">
<h2><span class="header-section-number">3.2</span> Logistic Regression Model</h2>
<p>We’ll start by writing up the data formally. For reference I’ll start with the standard linear model for regression just to get our bearings. We depict it as follows:</p>
<p><span class="math display">\[ \mu = b_0 + b_1*x_1 + b_2*x_2 \dots b_p*x_p \]</span> <span class="math display">\[ \mu = X\beta \]</span> <span class="math display">\[ y \sim \mathcal{N}(\mu, \sigma^2)\]</span></p>
<p>In the above <span class="math inline">\(\mu\)</span> is the linear predictor, the weighted combination of <span class="math inline">\(p\)</span> covariates <span class="math inline">\(x\)</span>, written two ways, one explicit and one using matrix notation, where <span class="math inline">\(X\)</span> is the model matrix and <span class="math inline">\(\beta\)</span> the vector of coefficients. The former is for those who are not familiar with matrix notation, as the latter can just be considered shorthand[^subscript]. The coefficients we wish to estimate are <span class="math inline">\(\beta\)</span>, and for the normal distribution we also need to estimate the variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>For binary target variables we do not assume the data generating process is a normal distribution, but instead we often consider a binomial as above. The Bernoulli distribution is a special case of the binomial for the situation where size=1, and might be more optimal to use for some modeling approaches (e.g. Stan). With logistic regression, the linear predictor is the <span class="emph">logit</span>, or log of the probability of the specific label of interest over the 1 minus that probability. Note that which label we refer to is arbitrary, e.g. whether you want the probability to regard a ‘yes’ outcome or ‘no’ is entirely up to you.</p>
<p>The logit is the name for the (natural) log of the odds, <span class="math inline">\(\pi/(1-\pi)\)</span>, i.e. the ratio of the probability of the event of interest, <span class="math inline">\(\pi\)</span>, to the probability of its non-occurrence. It theoretically ranges from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span> and is centered at zero, which is akin to a probability of .5. The logit is assumed to be some function of the covariates.</p>
<p><span class="math display">\[\begin{align}
\textrm{Logit}&amp;\: \vcenter{:}\mathord{=} \:\ln(\frac{\pi}{1-\pi}) \\
\textrm{Logit} &amp;= X\beta
\end{align}\]</span></p>
<p>The transformation function, or <span class="emph">link function</span>, of interest is the <span class="emph">logistic</span> link, hence the name logistic regression. Probabilities are inherently nonlinear, e.g. the change from .05 to .10 is a doubling of the probability, while that from .50 to .55 is only a 10% increase. To engage in a linear model of the sort we use in other modeling approaches, the logistic link transforms the probability response to the logit. Converting back to the probability scale requires the inverse logistic function, which might be depicted in different ways.</p>
<p><a id="binpred"></a> <span class="math display">\[\begin{align} 
\pi &amp;= \textrm{Logit}^{-1} \\
\pi &amp;= \frac{1}{1+e^{-XB}}, \textrm{or} \\
\pi &amp;= \frac{e^{XB}}{1+e^{XB}}
\end{align}\]</span></p>
<p>And finally, given the probability, we can get the likelihood of the the response given that probability.</p>
<p><span class="math display">\[\begin{align} 
y &amp;\sim \mathrm{Bin}(\pi, \mathrm{size}=1), \textrm{ or} \\
y &amp;\sim \mathrm{Bern}(\pi)
\end{align}\]</span></p>
<p>To make this clearer, let’s convert a value presumed to be on the logit scale to a probability. We’ll demonstrate the logit and inverse logit as well as the alternate ‘by-hand’ ways it might be depicted in various sources.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plogis</span>(<span class="dv">0</span>)                  <span class="co"># convert to probability</span></code></pre></div>
<pre><code>[1] 0.5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">log</span>(.<span class="dv">5</span>/(<span class="dv">1</span><span class="fl">-.5</span>))             <span class="co"># logit</span></code></pre></div>
<pre><code>[1] 0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qlogis</span>(.<span class="dv">5</span>)</code></pre></div>
<pre><code>[1] 0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plogis</span>(-<span class="dv">1</span>)                 </code></pre></div>
<pre><code>[1] 0.2689414</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plogis</span>(<span class="dv">1</span>)                  <span class="co"># 1 minus plogis(-1)</span></code></pre></div>
<pre><code>[1] 0.7310586</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span>/(<span class="dv">1</span>+<span class="kw">exp</span>(-<span class="dv">1</span>))</code></pre></div>
<pre><code>[1] 0.7310586</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="dv">1</span>)/(<span class="dv">1</span>+<span class="kw">exp</span>(<span class="dv">1</span>))</code></pre></div>
<pre><code>[1] 0.7310586</code></pre>
<p>Such a model is often called a logit model. However, calling a model by its link function seems odd to me for several reasons: there are several link functions one might use, the logit link function is used for other models (e.g. ordinal, neural nets), just calling it logit model doesn’t tell you how many categories are present, and we don’t do this with other models. Case in point, a very common alternative is the <span class="emph">probit</span> link function, which uses the cumulative normal distribution[^qnorm] to convert the probability scale to the logit. Practically anything that converts the linear predictor to <span class="math inline">\((0,1)\)</span> will technically work. However, calling it a ‘link’ model does not change what one thinks about the underlying response distribution in this setting.</p>
<p><span class="math display">\[ y \sim \mathrm{Bern}(\pi) \quad \scriptsize{\textrm{(a probit model, same as the logistic model)}} \]</span></p>
<p>An example of different link functions is shown in the next figure. There are sometimes reasons to prefer one over another, e.g. considering what one thinks about the tails of the distribution, and some can be seen as special cases of others (e.g. the <span class="emph">complementary log-log</span> is a special case of <span class="emph">generalized extreme value</span>). The choice becomes more important in multinomial and ordinal models.</p>
<p><img src="Categorical-Models_files/figure-html/links-1.png" width="672"  style="display: block; margin: auto;" /></p>
</div>
<p style="text-align: center;">
<a href="3-1-data.html"><button class="btn btn-default">Previous</button></a>
<a href="3-3-example.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
